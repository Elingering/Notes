# 05 | 数组：为什么很多编程语言中数组都从0开始编号？
## 如何实现随机访问？
数组（Array）是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。

第一是==线性表==（Linear List）。顾名思义，线性表就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/10/Snipaste_2020-04-10_10-42-28-1586491113296.png)
而与它相对立的概念是非线性表，比如二叉树、堆、图等。之所以叫非线性，是因为，在非线性表中，数据之间并不是简单的前后关系。
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/10/Snipaste_2020-04-10_10-42-35-1586491120249.png)
第二个是==连续的内存空间和相同类型的数据==。正是因为这两个限制，它才有了一个堪称“杀手锏”的特性：“随机访问”。

链表适合插入、删除，时间复杂度 O(1)；数组支持随机访问，根据下标随机访问的时间复杂度为 O(1)

## 低效的“插入”和“删除”
数组有序，插入的时间复杂度为O(n);
插入到数组末尾时间复杂度为O(1);
数组无序，插入到某个位置：把该位置的元素插入到末尾，新元素替换该位置的元素时间复杂度为O(1)；

删除也要保证数据的连续性，为了避免数据会被搬移多次次，我们可以先记录下已经删除的数据。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。

## 警惕数组的访问越界问题

## Tips
函数体内的局部变量存在栈上，且是连续压栈。在Linux进程的内存布局中，栈区在高地址空间，从高向低增长。变量i和arr在相邻地址，且i比arr的地址大，所以arr越界正好访问到i。当然，前提是i和arr元素同类型，否则那段代码仍是未决行为。

# 06 | 链表（上）：如何实现LRU缓存淘汰算法?
## 五花八门的链表结构
链表并不需要一块连续的内存空间，它通过“指针”将一组==零散的内存块==串联起来

三种最常见的链表结构，它们分别是：==单链表、双向链表和循环链表==。

## 链表 VS 数组性能大比拼
如果你的代码对内存的使用非常苛刻，那数组就更适合你。因为链表中的每个结点都需要消耗额外的存储空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍。而且，对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片，如果是Java 语言，就有可能会导致频繁的 GC（Garbage Collection，垃圾回收）。

# 07 | 链表（下）：如何轻松写出正确的链表代码？
## 技巧一：理解指针或引用的含义
将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。

## 技巧二：警惕指针丢失和内存泄漏
操作结点时，一定要注意操作的顺序

## 技巧三：利用哨兵简化实现难度
针对链表的插入、删除操作，需要对插入第一个结点和删除最后一个结点的情况进行特殊处理。

还记得如何表示一个空链表吗？head=null 表示链表中没有结点了。其中 head 表示头结点指
针，指向链表中的第一个结点。

如果我们引入哨兵结点，在任何时候，不管链表是不是空，head 指针都会一直指向这个哨兵结点。

我们也把这种有哨兵结点的链表叫带头链表。相反，没有哨兵结点的链表就叫作不带头链表。

哨兵结点是不存储数据的。因为哨兵结点一直存在，所以插入第一个结点和插入其他结点，删除最后一个结点和删除其他结点，都可以统一为相同的代码实现逻辑了。
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/10/Snipaste_2020-04-10_12-22-03-1586492537783.png)

## 技巧四：重点留意边界条件处理
我经常用来检查链表代码是否正确的边界条件有这样几个：
- 如果链表为空时，代码是否能正常工作？
- 如果链表只包含一个结点时，代码是否能正常工作？
- 如果链表只包含两个结点时，代码是否能正常工作？
- 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？

## 技巧五：举例画图，辅助思考
## 技巧六：多写多练，没有捷径
- 单链表反转
- 链表中环的检测
- 两个有序的链表合并
- 删除链表倒数第 n 个结点
- 求链表的中间结点

# 08 | 栈：如何实现浏览器的前进和后退功能？
## 如何理解“栈”？
叠盘子，==后进者先出，先进者后出，这就是典型的“栈”结构==。栈是一种“操作受限”的线性表

## 如何实现一个“栈”？
我们说==空间复杂度==的时候，是指除了**原本**的数据存储空间外，算法运行还需要**额外**的存储空间。

## 支持动态扩容的顺序栈
## 栈在函数调用中的应用
我们知道，操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构,用来存储函数调用时的**临时变量**。每进入一个**函数**，就会将临时变量作为一个栈帧**入栈**，当被调用函数执行完成，返回之后，将这个函数对应的栈帧**出栈**。

## 栈在表达式求值中的应用
实际上，编译器就是通过两个栈来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。我们从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较。

如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。

## 栈在括号匹配中的应用
我们用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹，“{”跟“}”匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。

## Tips
内存中的堆栈和数据结构堆栈不是一个概念，可以说内存中的堆栈是真实存在的物理区，数据结构中的堆栈是抽象的数据存储结构。

内存空间在逻辑上分为三部分：代码区、静态数据区和动态数据区，动态数据区又分为栈区和堆区。
- 代码区：存储方法体的二进制代码。高级调度（作业调度）、中级调度（内存调度）、低级调度（进程调度）控制代码区执行代码的切换。
- 静态数据区：存储全局变量、静态变量、常量，常量包括final修饰的常量和String常量。系统自动分配和回收。
- 栈区：存储运行方法的形参、局部变量、返回值。由系统自动分配和回收。
- 堆区：new一个对象的引用或地址存储在栈区，指向该对象存储在堆区中的真实数据。

# 09 | 队列：队列在线程池等有限资源池中的应用
## 如何理解“队列”？
先进者先出，这就是典型的“队列”。

## 顺序队列和链式队列
## 循环队列
最关键的是，==确定好队空和队满的判定条件==。

队空：head == tail
队满：(tail+1)%n=head

当队列满时，图中的 tail 指向的位置实际上是没有存储数据的。所以，循环队列会==浪费一个数组的存储空间==。

## 阻塞队列和并发队列
阻塞队列就是一个“生产者 - 消费者模型”！

==线程安全==的队列我们叫作并发队列。基于数组的循环队列，利用 CAS 原子操作，可以实现非常高效的并发队列（==无锁==）。

## Tips
实际上，对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队。

# 10 | 递归：如何用三行代码找到“最终推荐人”？
## 如何理解“递归”？
去的过程叫“递”，回来的过程叫“归”。

## 递归需要满足的三个条件
1. 一个问题的解可以分解为几个子问题的解
2. 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样
3. 存在递归终止条件

## 如何编写递归代码？
==写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。==

==编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。==

## 递归代码要警惕堆栈溢出
我们可以通过在代码中限制递归调用的最大深度的方式来解决这个问题。

## 递归代码要警惕重复计算
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/11/Snipaste_2020-04-11_11-11-54-1586577234505.png)
为了避免重复计算，我们可以通过一个数据结构（比如散列表）来保存已经求解过的 f(k)。当递归调用到 f(k) 时，先看下是否已经求解过了。如果是，则直接从散列表中取值返回，不需要重复计算。

## 怎么将递归代码改写为非递归代码？
所有的递归代码都可以改为这种迭代循环的非递归写法

但是这种思路实际上是将递归改为了“手动”递归，本质并没有变，而且也并没有解决前面讲到的某些问题，徒增了实现的复杂度。

# 11 | 排序（上）：为什么插入排序比冒泡排序更受欢迎？
最经典的、最常用的：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序。
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/11/Snipaste_2020-04-11_11-21-38-1586577223912.png)

## 如何分析一个“排序算法”？
- 排序算法的执行效率
1. 最好情况、最坏情况、平均情况时间复杂度
2. 时间复杂度的系数、常数 、低阶
3. 比较次数和交换（或移动）次数（基于比较的排序算法）
- 排序算法的内存消耗
==原地排序==算法，就是特指空间复杂度是 O(1) 的排序算法。
- 排序算法的稳定性
如果待排序的序列中存在值**相等的元素**，经过排序之后，相等元素之间**原有的先后顺序不变**。

## 冒泡排序（Bubble Sort）
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/11/Snipaste_2020-04-11_12-15-20-1586578750792.png)
冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作。

有序度是数组中具有有序关系的元素对的个数:a[i] <= a[j], 如果 i < j

逆序元素对：a[i] > a[j], 如果 i < j

满有序度:n*(n-1)/2

逆序度 = 满有序度 - 有序度

## 插入排序（Insertion Sort）
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/11/Snipaste_2020-04-11_12-15-33-1586578745623.png)
元素移动次数=逆序度

冒泡排序的数据交换要比插入排序的数据移动要复杂，冒泡排序需要3 个赋值操作，而插入排序只需要 1 个。

## 选择排序（Selection Sort）
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/11/Snipaste_2020-04-11_12-15-46-1586578737838.png)
冒泡排序、选择排序，可能就纯粹停留在理论的层面了，学习的目的也只是为了开拓思维，实际开发中应用并不多，但是插入排序还是挺有用的。
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/11/Snipaste_2020-04-11_12-11-19-1586578727981.png)

# 12 | 排序（下）：如何用快排思想在O(n)内查找第K大元素？
## 归并排序的原理
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/11/Snipaste_2020-04-11_14-45-27-1586587541139.png)
归并排序使用的就是==分治思想==。分治，顾名思义，就是分而治之，将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了。

分治算法一般都是用递归来实现的。分治是一种解决问题的处理思想，递归是一种编程技巧。

merge() 合并函数如果借助哨兵，代码就会简洁很多

## 归并排序的性能分析
稳定排序，时间复杂度O(nlogn)，空间复杂度O(n)，非原地排序。

## 快速排序的原理
非稳定排序，（可以）原地排序
#image#
可以发现，归并排序的处理过程是**由下到上**的，先处理子问题，然后再合并。而快排正好相反，它的处理过程是**由上到下**的，先分区，然后再处理子问题。归并排序虽然是稳定的、时间复杂度为 O(nlogn) 的排序算法，但是它是非原地排序算法。我们前面讲过，归并之所以是非原地排序算法，主要原因是合并函数无法在原地执行。快速排序通过设计巧妙的原地分区函数，可以实现原地排序，解决了归并排序占用太多内存的问题。

## 快速排序的性能分析
大部分情况下的时间复杂度都可以做到 O(nlogn)，只有在极端情况下，才会退化到 O(n^2^)。

## Tips
使用快排可以==在 O(n) 的时间复杂度内查找一个无序数组中的第 K 大元素==

# 13 | 线性排序：如何根据年龄给100万用户数据排序？
三种时间复杂度是 O(n) 的排序算法：桶排序、计数排序、基数排序。
## 桶排序（Bucket sort）
==桶排序比较适合用在外部排序中==。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。

## 计数排序（Counting sort）
计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。

## 基数排序（Radix sort）
基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到 O(n) 了。

## 小结
今天，我们学习了 3 种线性时间复杂度的排序算法，有桶排序、计数排序、基数排序。它们对要排序的数据都有比较苛刻的要求，应用不是非常广泛。但是如果数据特征比较符合这些排序算法的要求，应用这些算法，会非常高效，线性时间复杂度可以达到 O(n)。

桶排序和计数排序的排序思想是非常相似的，都是针对范围不大的数据，将数据划分成不同的桶来实现排序。基数排序要求数据可以划分成高低位，位之间有递进关系。比较两个数，我们只需要比较高位，高位相同的再比较低位。而且每一位的数据范围不能太大，因为基数排序算法需要借助桶排序或者计数排序来完成每一个位的排序工作。

# 14 | 排序优化：如何实现一个通用的、高性能的排序函数？
## 如何选择合适的排序算法？
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/28/Snipaste_2020-04-28_13-41-14-1588052485845.png)
## 如何优化快速排序？
合理选择分区点：x数取中法，随机法。

# 15 | 二分查找（上）：如何用最省内存的方式实现快速查找功能？
## 二分查找的递归与非递归实现
1. 循环退出条件
注意是 low<=high，而不是 low<high。
2. mid 的取值
实际上，mid=(low+high)/2 这种写法是有问题的。因为如果 low 和 high 比较大的话，两者之和就有可能会溢出。改进的方法是将 mid 的计算方式写成 low+(high-low)/2。更进一步，如果要将性能优化到极致的话，我们可以将这里的除以 2 操作转化成位运算 low+((high-low)>>1)。因为相比除法运算来说，计算机处理位运算要快得多。
3. low 和 high 的更新
low=mid+1，high=mid-1。注意这里的 +1 和 -1，如果直接写成 low=mid 或者 high=mid，就
可能会发生死循环。比如，当 high=3，low=3 时，如果a[3]不等于value，就会导致一直循环不退出。

## 二分查找应用场景的局限性
二分查找虽然性能比较优秀，但应用场景也比较有限。底层必须==依赖数组==，并且还要求数据是==有序==的。对于较小规模的数据查找，我们直接使用顺序遍历就可以了，二分查找的优势并不明显。数据量太大又可能导致数组空间（连续空间）不足。二分查找更==适合处理静态数据==，也就是没有频繁的数据插入、删除操作。

# 16 | 二分查找（下）：如何快速定位IP对应的省份地址？
IP 地址可以转化为 32 位的整型数。
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/28/Snipaste_2020-04-28_14-15-00-1588055179862.png)

## 小结
凡是用二分查找能解决的，绝大部分我们更倾向于用散列表或者二叉查找树。即便是二分查找在内存使用上更节省，但是毕竟内存如此紧缺的情况并不多。

二分查找更适合用在“近似”查找问题，在这类问题上，二分查找的优势更加明显。比如今天讲的这几种变体问题，用其他数据结构，比如散列表、二叉树，就比较难实现了。

# 17 | 跳表：为什么Redis一定要用跳表来实现有序集合？
## 如何理解“跳表”？
这种链表加多级索引的结构，就是跳表。

## 用跳表查询到底有多快？
时间复杂度就是 O(logn)

## 跳表是不是很浪费内存？
实际上，在软件开发中，我们不必太在意索引占用的额外空间。在讲数据结构和算法时，我们习惯性地把要处理的数据看成整数，但是在实际的软件开发中，原始链表中存储的有可能是很大的对象，而索引结点只需要存储关键值和几个指针，并不需要存储对象，所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略了。

## 跳表索引动态更新
作为一种动态数据结构，我们需要某种手段来维护索引与原始链表大小之间的平衡，也就是说，如果链表中结点多了，索引结点就相应地增加一些，避免复杂度退化，以及查找、插入、删除操作性能下降。

我们通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值 K，那我们就将这个结点添加到第一级到第 K 级这 K 级索引中。

## 小结
跳表是一种动态数据结构，支持快速的插入、删除、
查找操作，时间复杂度都是 O(logn)。

跳表的空间复杂度是 O(n)。不过，跳表的实现非常灵活，可以通过改变索引构建策略，有效平衡执行效率和内存消耗。虽然跳表的代码实现并不简单，但是作为一种动态数据结构，比起红黑树来说，实现要简单多了。所以很多时候，我们为了代码的简单、易读，比起红黑树，我们更倾向用跳表。

# 18 | 散列表（上）：Word文档中的单词拼写检查功能是如何实现的？
## 散列思想
散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。

## 散列函数
三点散列函数设计的基本要求：
1. 散列函数计算得到的散列值是一个非负整数；
2. 如果 key1 = key2，那 hash(key1) == hash(key2)；
3. 如果 key1 ≠ key2，那 hash(key1) ≠ hash(key2)。

## 散列冲突
1. 开放寻址法
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/28/Snipaste_2020-04-28_15-12-10-1588058055077.png)
2. 链表法
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/28/Snipaste_2020-04-28_15-12-28-1588058072051.png)

# 19 | 散列表（中）：如何打造一个工业级水平的散列表？
## 如何设计散列函数？
散列函数的设计不能太复杂。
散列函数生成的值要尽可能随机并且均匀分布。

## 装载因子过大了怎么办？
装载因子越大，说明散列表中的元素越多，空闲位置越少，散列冲突的概率就越大。
==散列表的装载因子 = 填入表中的元素个数 / 散列表的长度==

## 如何避免低效地扩容？
申请一个新空间，当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。如此反复，直到老的数据迁移完。

## 如何选择冲突解决方法？
1. 开放寻址法
当数据量比较小、装载因子小的时候，适合采用开放寻址法。这也是 Java中的ThreadLocalMap使用开放寻址法解决散列冲突的原因。
2. 链表法
基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树/跳表代替链表。

# 20 | 散列表（下）：为什么散列表和链表经常会一起使用？
散列表这种数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的。也就说，它无法支持按照某种顺序快速地遍历数据。如果希望按照顺序遍历散列表中的数据，那我们需要将散列表中的数据拷贝到数组中，然后排序，再遍历。

因为散列表是动态数据结构，不停地有数据的插入、删除，所以每当我们希望按顺序遍历散列表中的数据的时候，都需要先排序，那效率势必会很低。为了解决这个问题，我们将散列表和链表（或者跳表）结合在一起使用。

# 21 | 哈希算法（上）：如何防止数据库中的用户信息被脱库?
## 什么是哈希算法?
将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是哈希算法，而通过原始数据映射之后得到的二进制值串就是哈希值。

设计一个优秀的哈希算法:
- 从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法）；
- 对输入数据非常敏感，哪怕原始数据只修改了一个 Bit，最后得到的哈希值也大不相同；
- 散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小；
- 哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值。

## 哈希算法的应用
应用一：安全加密
应用二：唯一标识
应用三：数据校验
应用四：散列函数

# 22 | 哈希算法（下）：哈希算法在分布式系统中有哪些应用？
## 应用五：负载均衡
==我们可以通过哈希算法，对客户端 IP 地址或者会话 ID 计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号==。 这样，我们就可以把同一个 IP 过来的所有请求，都路由到同一个后端服务器上。

## 应用六：数据分片
1. 如何统计“搜索关键词”出现的次数？
==我们可以先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度==。具体的思路是这样的：为了提高处理的速度，我们用 n 台机器并行处理。我们从搜索记录的日志文件中，依次读出每个搜索关键词，并且通过哈希函数计算哈希值，然后再跟 n 取模，最终得到的值，就是应该被分配到的机器编号。

2. 如何快速判断图片是否在图库中？

## 应用七：分布式存储
我们需要一种方法，使得在新加入一个机器后，并不需要做大量的数据搬移。这时候，==一致性哈希算法==就要登场了。

假设我们有 k 个机器，数据的哈希值的范围是 [0, MAX]。我们将整个范围划分成 m 个小区间（m 远大于 k），每个机器负责 m/k 个小区间。当有新机器加入的时候，我们就将某几个小区间的数据，从原来的机器中搬移到新的机器中。这样，既不用全部重新哈希、搬移数据，也保持了各个机器上数据数量的均衡。

一致性哈希算法的基本思想就是这么简单。除此之外，它还会借助一个虚拟的环和虚拟结点，更加优美地实现出来。

# 23 | 二叉树基础（上）：什么样的二叉树适合用数组来存储？
## 树（Tree）
A 节点就是 B 节点的**父节点**，B 节点是 A 节点的**子节点**。B、C、D 这三个节点的父节点是同一个节点，所以它们之间互称为**兄弟节点**。我们把没有父节点的节点叫作**根节点**，也就是图中的节点 E。我们把没有子节点的节点叫作**叶子节点**或者**叶节点**，比如图中的 G、H、I、J、K、L 都是叶子节点。
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/30/Snipaste_2020-04-30_17-02-24-1588237431466.png)
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/30/Snipaste_2020-04-30_17-03-01-1588237440203.png)

## 二叉树（Binary Tree）
二叉树，顾名思义，每个节点==最多有==两个“叉”，也就是两个子节点，分别是左子节点和右子节点。

叶子节点==全都在最底层==，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫作**满二叉树**。

叶子节点都在最底下两层，最后一层的叶子节点都==靠左排列==，并且除了最后一层，其他层的节点个数都要达到==最大==，这种二叉树叫作**完全二叉树**。

如何表示（或者存储）一棵二叉树？
一种是基于指针或者引用的二叉链式存储法，一种是基于数组的顺序存储法。
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/30/Snipaste_2020-04-30_17-11-08-1588238107649.png)
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/30/Snipaste_2020-04-30_17-14-23-1588238113916.png)
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/30/Snipaste_2020-04-30_17-14-31-1588238119411.png)
堆其实就是一种完全二叉树，最常用的存储方式就是数组。

## 二叉树的遍历
经典的方法有三种，前序遍历、中序遍历和后序遍历。其中，前、中、后序，表示的是节点与它的左右子树节点遍历==打印的先后顺序==。
- 前序遍历是指，对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树。（中-左-右）
- 中序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树。（左-中-右）
- 后序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身。（左-右-中）
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/30/Snipaste_2020-04-30_17-20-53-1588238546797.png)

实际上，二叉树的前、中、后序遍历就是一个递归的过程。

从我前面画的前、中、后序遍历的顺序图，可以看出来，每个节点==最多会被访问两次==，所以遍历操作的时间复杂度，跟节点的个数 n 成正比，也就是说二叉树遍历的**时间复杂度**是 O(n)。

树还可以按层次遍历。树的层次遍历，其实也是一个广度优先的遍历算法。

# 24 | 二叉树基础（下）：有了如此高效的散列表，为什么还需要二叉树？
==二叉查找树==最大的特点就是，支持动态数据集合的快速插入、删除、查找操作。

## 二叉查找树（Binary Search Tree）
**二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值**。

- 二叉查找树的查找操作
- 二叉查找树的插入操作
- 二叉查找树的删除操作（三种情况：没有子节点，一个子节点，两个子节点）
- 二叉查找树的其他操作（**快速地查找最大节点和最小节点、前驱节点和后继节点**。）

一个重要的特性，就是==中序遍历二叉查找树，可以输出有序的数据序列，时间复杂度是 O(n)，非常高效==。

## 支持重复数据的二叉查找树
- 第一种方法比较容易。二叉查找树中每一个节点不仅会存储一个数据，因此我们通过链表和支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上。
- 第二种方法比较不好理解，不过更加优雅。每个节点仍然只存储一个数据。在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的右子树，也就是说，把这个新插入的数据当作大于这个节点的值来处理。

## 二叉查找树的时间复杂度分析
最坏：O(n)
平衡：O(logn)

## 散列表与二叉树的对比
- 第一，散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。而对于二叉查找树来说，我们只需要中序遍历，就可以在 O(n) 的时间复杂度内，输出有序的数据序列。
- 第二，散列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定，尽管二叉查找树的性能不稳定，但是在工程中，我们最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在O(logn)。
- 第三，笼统地来说，尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比 logn 小，所以实际的查找速度可能不一定比 O(logn) 快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。
- 第四，散列表的构造比二叉查找树要复杂，需要考虑的东西很多。比如散列函数的设计、冲突解决办法、扩容、缩容等。平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定。
- 最后，为了避免过多的散列冲突，散列表装载因子不能太大，特别是基于开放寻址法解决冲突的散列表，不然会浪费一定的存储空间。

# 25 | 红黑树（上）：为什么工程中都用红黑树这种二叉树？
## 什么是“平衡二叉查找树”？
平衡二叉树的严格定义是这样的：二叉树中任意一个节点的左右子树的高度相差不能大于 1。

平衡二叉查找树中“平衡”的意思，其实就是让整棵树左右看起来比较“对称”、比较“平衡”，不要出现左子树很高、右子树很矮的情况。这样就能让整棵树的高度相对来说低一些，相应的插入、删除、查找等操作的效率高一些。

## 如何定义一棵“红黑树”？
红黑树中的节点，一类被标记为黑色，一类被标记为红色。
- 根节点是黑色的；
- 每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据；
- 任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的；
- 每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点；

## 为什么说红黑树是“近似平衡”的？
平衡二叉查找树的初衷，是为了解决二叉查找树因为动态更新导致的性能退化问题。所以，“平衡”的意思可以等价为性能不退化。“近似平衡”就等价为性能不会退化的太严重。

## 小结
红黑树是一种平衡二叉查找树。它是为了解决普通二叉查找树在数据更新的过程中，复杂度退化的问题而产生的。红黑树的高度近似 log~2~n，所以它是近似平衡，插入、删除、查找操作的时间复杂度都是 O(logn)。

因为红黑树是一种性能非常稳定的二叉查找树，所以，在工程中，但凡是用到动态插入、删除、查找数据的场景，都可以用到它。不过，它实现起来比较复杂，如果自己写代码实现，难度会有些高，这个时候，我们其实更倾向用跳表来替代它。

# 26 | 红黑树（下）：掌握这些技巧，你也可以实现一个红黑树

# 27 | 递归树：如何借助树来求解递归算法的时间复杂度？

# 28 | 堆和堆排序：为什么说堆排序没有快速排序快？
## 如何理解“堆”？
- 堆是一个完全二叉树；
- 堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。（大顶堆，小顶堆）

## 往堆中插入一个元素
调整元素位置，让其重新满足堆的特性，这个过程我们起了一个名字，就叫作**堆化**。

从下往上堆化
##image##

## 删除堆顶元素
从上往下堆化
##image##

往堆中插入一个元素和删除堆顶元素的时间复杂度都是O(logn)。

## 如何基于堆实现排序？
1. 建堆（建堆的时间复杂度就是O(n)。）
- 第一种是借助我们前面讲的，在堆中插入一个元素的思路。尽管数组中包含 个数据，但是我们可以假设，起初堆中只包含一个数据，就是下标为 的数据。然后，我们调用前面讲的插入操作，将下标从 到 的数据依次插入到堆中。这样我们就将包含 个数据的数组，组织成了
堆。
- 第二种实现思路，跟第一种截然相反，也是我这里要详细讲的。第一种建堆思路的处理过程是从前往后处理数组数据，并且每个数据插入堆中时，都是从下往上堆化。而第二种实现思路，是从后往前处理数组，并且每个数据都是从上往下堆化。
2. 排序
使用从堆顶删除元素的逻辑。

## 小结
为什么快速排序要比堆排序性能好？
- 第一点，堆排序数据访问的方式没有快速排序友好。
- 第二点，对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序。

# 29 | 堆的应用：如何快速获取到Top 10最热门的搜索关键词？
堆这种数据结构几个非常重要的应用：==优先级队列、求 Top K 和求中位数==。

## 堆的应用一：优先级队列
1. 合并有序小文件
2. 高性能定时器

## 堆的应用二：利用堆求 Top K
我们可以维护一个==大小为 K 的小顶堆==，顺序遍历数组，从数组中取出取数据与堆顶元素比较。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理，继续遍历数组。这样等数组中的数据都遍历完之后，堆中的数据就是前 K 大数据了。


# 42 