# 05 | 数组：为什么很多编程语言中数组都从0开始编号？
## 如何实现随机访问？
数组（Array）是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。

第一是==线性表==（Linear List）。顾名思义，线性表就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/10/Snipaste_2020-04-10_10-42-28-1586491113296.png)
而与它相对立的概念是非线性表，比如二叉树、堆、图等。之所以叫非线性，是因为，在非线性表中，数据之间并不是简单的前后关系。
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/10/Snipaste_2020-04-10_10-42-35-1586491120249.png)
第二个是==连续的内存空间和相同类型的数据==。正是因为这两个限制，它才有了一个堪称“杀手锏”的特性：“随机访问”。

链表适合插入、删除，时间复杂度 O(1)；数组支持随机访问，根据下标随机访问的时间复杂度为 O(1)

## 低效的“插入”和“删除”
数组有序，插入的时间复杂度为O(n);
插入到数组末尾时间复杂度为O(1);
数组无序，插入到某个位置：把该位置的元素插入到末尾，新元素替换该位置的元素时间复杂度为O(1)；

删除也要保证数据的连续性，为了避免数据会被搬移多次次，我们可以先记录下已经删除的数据。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。

## 警惕数组的访问越界问题

## Tips
函数体内的局部变量存在栈上，且是连续压栈。在Linux进程的内存布局中，栈区在高地址空间，从高向低增长。变量i和arr在相邻地址，且i比arr的地址大，所以arr越界正好访问到i。当然，前提是i和arr元素同类型，否则那段代码仍是未决行为。

# 06 | 链表（上）：如何实现LRU缓存淘汰算法?
## 五花八门的链表结构
链表并不需要一块连续的内存空间，它通过“指针”将一组==零散的内存块==串联起来

三种最常见的链表结构，它们分别是：==单链表、双向链表和循环链表==。

## 链表 VS 数组性能大比拼
如果你的代码对内存的使用非常苛刻，那数组就更适合你。因为链表中的每个结点都需要消耗额外的存储空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍。而且，对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片，如果是Java 语言，就有可能会导致频繁的 GC（Garbage Collection，垃圾回收）。

# 07 | 链表（下）：如何轻松写出正确的链表代码？
## 技巧一：理解指针或引用的含义
将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。

## 技巧二：警惕指针丢失和内存泄漏
操作结点时，一定要注意操作的顺序

## 技巧三：利用哨兵简化实现难度
针对链表的插入、删除操作，需要对插入第一个结点和删除最后一个结点的情况进行特殊处理。

还记得如何表示一个空链表吗？head=null 表示链表中没有结点了。其中 head 表示头结点指
针，指向链表中的第一个结点。

如果我们引入哨兵结点，在任何时候，不管链表是不是空，head 指针都会一直指向这个哨兵结点。

我们也把这种有哨兵结点的链表叫带头链表。相反，没有哨兵结点的链表就叫作不带头链表。

哨兵结点是不存储数据的。因为哨兵结点一直存在，所以插入第一个结点和插入其他结点，删除最后一个结点和删除其他结点，都可以统一为相同的代码实现逻辑了。
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/10/Snipaste_2020-04-10_12-22-03-1586492537783.png)

## 技巧四：重点留意边界条件处理
我经常用来检查链表代码是否正确的边界条件有这样几个：
- 如果链表为空时，代码是否能正常工作？
- 如果链表只包含一个结点时，代码是否能正常工作？
- 如果链表只包含两个结点时，代码是否能正常工作？
- 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？

## 技巧五：举例画图，辅助思考
## 技巧六：多写多练，没有捷径
- 单链表反转
- 链表中环的检测
- 两个有序的链表合并
- 删除链表倒数第 n 个结点
- 求链表的中间结点

# 08 | 栈：如何实现浏览器的前进和后退功能？
## 如何理解“栈”？
叠盘子，==后进者先出，先进者后出，这就是典型的“栈”结构==。栈是一种“操作受限”的线性表

## 如何实现一个“栈”？
我们说==空间复杂度==的时候，是指除了**原本**的数据存储空间外，算法运行还需要**额外**的存储空间。

## 支持动态扩容的顺序栈
## 栈在函数调用中的应用
我们知道，操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构,用来存储函数调用时的**临时变量**。每进入一个**函数**，就会将临时变量作为一个栈帧**入栈**，当被调用函数执行完成，返回之后，将这个函数对应的栈帧**出栈**。

## 栈在表达式求值中的应用
实际上，编译器就是通过两个栈来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。我们从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较。

如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。

## 栈在括号匹配中的应用
我们用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹，“{”跟“}”匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。

## Tips
内存中的堆栈和数据结构堆栈不是一个概念，可以说内存中的堆栈是真实存在的物理区，数据结构中的堆栈是抽象的数据存储结构。

内存空间在逻辑上分为三部分：代码区、静态数据区和动态数据区，动态数据区又分为栈区和堆区。
- 代码区：存储方法体的二进制代码。高级调度（作业调度）、中级调度（内存调度）、低级调度（进程调度）控制代码区执行代码的切换。
- 静态数据区：存储全局变量、静态变量、常量，常量包括final修饰的常量和String常量。系统自动分配和回收。
- 栈区：存储运行方法的形参、局部变量、返回值。由系统自动分配和回收。
- 堆区：new一个对象的引用或地址存储在栈区，指向该对象存储在堆区中的真实数据。

# 09 | 队列：队列在线程池等有限资源池中的应用
## 如何理解“队列”？
先进者先出，这就是典型的“队列”。

## 顺序队列和链式队列
## 循环队列
最关键的是，==确定好队空和队满的判定条件==。

队空：head == tail
队满：(tail+1)%n=head

当队列满时，图中的 tail 指向的位置实际上是没有存储数据的。所以，循环队列会==浪费一个数组的存储空间==。

## 阻塞队列和并发队列
阻塞队列就是一个“生产者 - 消费者模型”！

==线程安全==的队列我们叫作并发队列。基于数组的循环队列，利用 CAS 原子操作，可以实现非常高效的并发队列（==无锁==）。

## Tips
实际上，对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队。

# 10 | 递归：如何用三行代码找到“最终推荐人”？
## 如何理解“递归”？
去的过程叫“递”，回来的过程叫“归”。

## 递归需要满足的三个条件
1. 一个问题的解可以分解为几个子问题的解
2. 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样
3. 存在递归终止条件

## 如何编写递归代码？
==写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。==

==编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。==

## 递归代码要警惕堆栈溢出
我们可以通过在代码中限制递归调用的最大深度的方式来解决这个问题。

## 递归代码要警惕重复计算
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/11/Snipaste_2020-04-11_11-11-54-1586577234505.png)
为了避免重复计算，我们可以通过一个数据结构（比如散列表）来保存已经求解过的 f(k)。当递归调用到 f(k) 时，先看下是否已经求解过了。如果是，则直接从散列表中取值返回，不需要重复计算。

## 怎么将递归代码改写为非递归代码？
所有的递归代码都可以改为这种迭代循环的非递归写法

但是这种思路实际上是将递归改为了“手动”递归，本质并没有变，而且也并没有解决前面讲到的某些问题，徒增了实现的复杂度。

# 11 | 排序（上）：为什么插入排序比冒泡排序更受欢迎？
最经典的、最常用的：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序。
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/11/Snipaste_2020-04-11_11-21-38-1586577223912.png)

## 如何分析一个“排序算法”？
- 排序算法的执行效率
1. 最好情况、最坏情况、平均情况时间复杂度
2. 时间复杂度的系数、常数 、低阶
3. 比较次数和交换（或移动）次数（基于比较的排序算法）
- 排序算法的内存消耗
==原地排序==算法，就是特指空间复杂度是 O(1) 的排序算法。
- 排序算法的稳定性
如果待排序的序列中存在值**相等的元素**，经过排序之后，相等元素之间**原有的先后顺序不变**。

## 冒泡排序（Bubble Sort）
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/11/Snipaste_2020-04-11_12-15-20-1586578750792.png)
冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作。

有序度是数组中具有有序关系的元素对的个数:a[i] <= a[j], 如果 i < j

逆序元素对：a[i] > a[j], 如果 i < j

满有序度:n*(n-1)/2

逆序度 = 满有序度 - 有序度

## 插入排序（Insertion Sort）
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/11/Snipaste_2020-04-11_12-15-33-1586578745623.png)
元素移动次数=逆序度

冒泡排序的数据交换要比插入排序的数据移动要复杂，冒泡排序需要3 个赋值操作，而插入排序只需要 1 个。

## 选择排序（Selection Sort）
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/11/Snipaste_2020-04-11_12-15-46-1586578737838.png)
冒泡排序、选择排序，可能就纯粹停留在理论的层面了，学习的目的也只是为了开拓思维，实际开发中应用并不多，但是插入排序还是挺有用的。
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/11/Snipaste_2020-04-11_12-11-19-1586578727981.png)

# 12 | 排序（下）：如何用快排思想在O(n)内查找第K大元素？
## 归并排序的原理
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/11/Snipaste_2020-04-11_14-45-27-1586587541139.png)
归并排序使用的就是==分治思想==。分治，顾名思义，就是分而治之，将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了。

分治算法一般都是用递归来实现的。分治是一种解决问题的处理思想，递归是一种编程技巧。

merge() 合并函数如果借助哨兵，代码就会简洁很多

## 归并排序的性能分析
稳定排序，时间复杂度O(nlogn)，空间复杂度O(n)，非原地排序。

## 快速排序的原理
非稳定排序，（可以）原地排序
#image#
可以发现，归并排序的处理过程是**由下到上**的，先处理子问题，然后再合并。而快排正好相反，它的处理过程是**由上到下**的，先分区，然后再处理子问题。归并排序虽然是稳定的、时间复杂度为 O(nlogn) 的排序算法，但是它是非原地排序算法。我们前面讲过，归并之所以是非原地排序算法，主要原因是合并函数无法在原地执行。快速排序通过设计巧妙的原地分区函数，可以实现原地排序，解决了归并排序占用太多内存的问题。

## 快速排序的性能分析
大部分情况下的时间复杂度都可以做到 O(nlogn)，只有在极端情况下，才会退化到 O(n^2^)。

## Tips
使用快排可以==在 O(n) 的时间复杂度内查找一个无序数组中的第 K 大元素==

# 13 | 线性排序：如何根据年龄给100万用户数据排序？
三种时间复杂度是 O(n) 的排序算法：桶排序、计数排序、基数排序。
## 桶排序（Bucket sort）
==桶排序比较适合用在外部排序中==。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。

## 计数排序（Counting sort）
计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。

## 基数排序（Radix sort）
基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到 O(n) 了。

## 小结
今天，我们学习了 3 种线性时间复杂度的排序算法，有桶排序、计数排序、基数排序。它们对要排序的数据都有比较苛刻的要求，应用不是非常广泛。但是如果数据特征比较符合这些排序算法的要求，应用这些算法，会非常高效，线性时间复杂度可以达到 O(n)。

桶排序和计数排序的排序思想是非常相似的，都是针对范围不大的数据，将数据划分成不同的桶来实现排序。基数排序要求数据可以划分成高低位，位之间有递进关系。比较两个数，我们只需要比较高位，高位相同的再比较低位。而且每一位的数据范围不能太大，因为基数排序算法需要借助桶排序或者计数排序来完成每一个位的排序工作。

# 14 | 排序优化：如何实现一个通用的、高性能的排序函数？
## 如何选择合适的排序算法？
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/28/Snipaste_2020-04-28_13-41-14-1588052485845.png)
## 如何优化快速排序？
合理选择分区点：x数取中法，随机法。

# 15 | 二分查找（上）：如何用最省内存的方式实现快速查找功能？
## 二分查找的递归与非递归实现
1. 循环退出条件
注意是 low<=high，而不是 low<high。
2. mid 的取值
实际上，mid=(low+high)/2 这种写法是有问题的。因为如果 low 和 high 比较大的话，两者之和就有可能会溢出。改进的方法是将 mid 的计算方式写成 low+(high-low)/2。更进一步，如果要将性能优化到极致的话，我们可以将这里的除以 2 操作转化成位运算 low+((high-low)>>1)。因为相比除法运算来说，计算机处理位运算要快得多。
3. low 和 high 的更新
low=mid+1，high=mid-1。注意这里的 +1 和 -1，如果直接写成 low=mid 或者 high=mid，就
可能会发生死循环。比如，当 high=3，low=3 时，如果a[3]不等于value，就会导致一直循环不退出。

## 二分查找应用场景的局限性
二分查找虽然性能比较优秀，但应用场景也比较有限。底层必须==依赖数组==，并且还要求数据是==有序==的。对于较小规模的数据查找，我们直接使用顺序遍历就可以了，二分查找的优势并不明显。数据量太大又可能导致数组空间（连续空间）不足。二分查找更==适合处理静态数据==，也就是没有频繁的数据插入、删除操作。

# 16 | 二分查找（下）：如何快速定位IP对应的省份地址？
IP 地址可以转化为 32 位的整型数。
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/28/Snipaste_2020-04-28_14-15-00-1588055179862.png)

## 小结
凡是用二分查找能解决的，绝大部分我们更倾向于用散列表或者二叉查找树。即便是二分查找在内存使用上更节省，但是毕竟内存如此紧缺的情况并不多。

二分查找更适合用在“近似”查找问题，在这类问题上，二分查找的优势更加明显。比如今天讲的这几种变体问题，用其他数据结构，比如散列表、二叉树，就比较难实现了。

# 17 | 跳表：为什么Redis一定要用跳表来实现有序集合？
## 如何理解“跳表”？
这种链表加多级索引的结构，就是跳表。

## 用跳表查询到底有多快？
时间复杂度就是 O(logn)

## 跳表是不是很浪费内存？
实际上，在软件开发中，我们不必太在意索引占用的额外空间。在讲数据结构和算法时，我们习惯性地把要处理的数据看成整数，但是在实际的软件开发中，原始链表中存储的有可能是很大的对象，而索引结点只需要存储关键值和几个指针，并不需要存储对象，所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略了。

## 跳表索引动态更新
作为一种动态数据结构，我们需要某种手段来维护索引与原始链表大小之间的平衡，也就是说，如果链表中结点多了，索引结点就相应地增加一些，避免复杂度退化，以及查找、插入、删除操作性能下降。

我们通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值 K，那我们就将这个结点添加到第一级到第 K 级这 K 级索引中。

## 小结
跳表是一种动态数据结构，支持快速的插入、删除、
查找操作，时间复杂度都是 O(logn)。

跳表的空间复杂度是 O(n)。不过，跳表的实现非常灵活，可以通过改变索引构建策略，有效平衡执行效率和内存消耗。虽然跳表的代码实现并不简单，但是作为一种动态数据结构，比起红黑树来说，实现要简单多了。所以很多时候，我们为了代码的简单、易读，比起红黑树，我们更倾向用跳表。

# 18 | 散列表（上）：Word文档中的单词拼写检查功能是如何实现的？
## 散列思想
散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。

## 散列函数
三点散列函数设计的基本要求：
1. 散列函数计算得到的散列值是一个非负整数；
2. 如果 key1 = key2，那 hash(key1) == hash(key2)；
3. 如果 key1 ≠ key2，那 hash(key1) ≠ hash(key2)。

## 散列冲突
1. 开放寻址法
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/28/Snipaste_2020-04-28_15-12-10-1588058055077.png)
2. 链表法
![title](https://raw.githubusercontent.com/Elingering/note-images/master/gitnote/2020/04/28/Snipaste_2020-04-28_15-12-28-1588058072051.png)

# 19 | 散列表（中）：如何打造一个工业级水平的散列表？
## 如何设计散列函数？
散列函数的设计不能太复杂。
散列函数生成的值要尽可能随机并且均匀分布。

## 装载因子过大了怎么办？
装载因子越大，说明散列表中的元素越多，空闲位置越少，散列冲突的概率就越大。
==散列表的装载因子 = 填入表中的元素个数 / 散列表的长度==

## 如何避免低效地扩容？
申请一个新空间，当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。如此反复，直到老的数据迁移完。

## 如何选择冲突解决方法？
1. 开放寻址法
当数据量比较小、装载因子小的时候，适合采用开放寻址法。这也是 Java中的ThreadLocalMap使用开放寻址法解决散列冲突的原因。
2. 链表法
基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树/跳表代替链表。

# 20 | 散列表（下）：为什么散列表和链表经常会一起使用？
散列表这种数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的。也就说，它无法支持按照某种顺序快速地遍历数据。如果希望按照顺序遍历散列表中的数据，那我们需要将散列表中的数据拷贝到数组中，然后排序，再遍历。

因为散列表是动态数据结构，不停地有数据的插入、删除，所以每当我们希望按顺序遍历散列表中的数据的时候，都需要先排序，那效率势必会很低。为了解决这个问题，我们将散列表和链表（或者跳表）结合在一起使用。

# 21 | 哈希算法（上）：如何防止数据库中的用户信息被脱库？
## 什么是哈希算法？





# 22 | 哈希算法（下）：哈希算法在分布式系统中有哪些应用？

# 42 